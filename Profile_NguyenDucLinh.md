### Professional Summary
Data Engineer and Analyst with over two years of hands-on experience in the data and AI industry, specializing in designing scalable ETL/ELT pipelines, developing analytics solutions, and applying machine learning techniques to drive business outcomes. Expertise spans data processing, visualization, and AI-driven optimizations, with a proven track record of automating workflows, enhancing data governance, and delivering actionable insights that support strategic decision-making. Demonstrated ability to collaborate across cross-functional teams, from engineering and product to customer service and operations, while adapting to evolving business needs. Recognized for innovative contributions in generative AI, including award-winning projects that improved operational efficiency and user experiences. Committed to leveraging data for problem-solving, with a focus on multi-cloud environments and open-source tools to ensure flexibility and scalability.

### Work Experience

#### Data Analyst, Customer Service Chatbot Team  
MoMo (M_Service)  
Ho Chi Minh City, Vietnam  
February 2025 – Present  

In the Customer Service Chatbot team at MoMo, a leading fintech company in Vietnam, I focused on data analytics and AI applications to enhance customer support systems. The team's primary objective was to optimize chatbot performance, reduce response errors, and improve overall customer satisfaction in a high-volume digital service environment. This role involved analyzing large datasets from customer interactions, applying AI models for error detection and categorization, and automating monitoring processes to enable proactive issue resolution. My contributions helped address key business challenges, such as inconsistent chatbot responses leading to customer frustration and manual oversight burdens on support teams, by integrating advanced AI techniques and fostering data-driven improvements.

- Conducted in-depth analysis of chatbot response errors, identifying root causes including data inconsistencies, NLP logic gaps, and process inefficiencies. Proposed and implemented systematic optimizations, such as refined training datasets and logic adjustments, which helped elevate customer satisfaction scores and aligned with annual service quality targets ahead of schedule.
- Developed real-time performance monitoring dashboards using Looker Studio, incorporating key metrics like response accuracy, resolution time, and user feedback trends. These dashboards provided stakeholders with immediate visibility into chatbot effectiveness, enabling quicker interventions and reducing the need for manual reviews.
- Implemented automated alert systems for detecting data anomalies and emerging customer issues on an hourly basis, integrating tools like Apache Airflow for scheduling and n8n for workflow automation. This proactive approach minimized downtime and supported faster resolution of service disruptions, alleviating workload pressures on customer service agents.
- Applied advanced AI and NLP models for sentiment analysis, issue categorization, and content summarization of customer queries. Researched and integrated techniques such as Prompt Engineering, Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), and knowledge base enhancements to refine chatbot capabilities, resulting in more contextually accurate responses and improved handling of complex inquiries.
- Automated query scheduling, task distribution, and reporting processes to streamline operations, ensuring seamless integration with existing customer service workflows. Collaborated closely with NLP specialists and customer service leads to define KPIs, such as customer satisfaction (CSAT) and first-contact resolution rates, and iterated on models based on performance data.
- Participated in cross-functional projects to align chatbot enhancements with broader business goals, including user retention and service scalability. This role marked a progression in my career toward AI-centric analytics, building on prior data engineering foundations to deliver solutions that directly impacted end-user experiences in a dynamic fintech setting.

Through this experience, I grew from handling technical implementations to leading AI-driven initiatives, gaining deeper insights into user-centric data applications and the importance of iterative model refinement in real-world deployments.

#### Data Analyst Trainee, Corporate Data Office (CDO) Team  
MoMo (M_Service)  
Ho Chi Minh City, Vietnam  
July 2024 – January 2025  

As part of MoMo's Corporate Data Office team, I concentrated on data engineering tasks to support enterprise-wide data infrastructure. The CDO team was responsible for building and maintaining robust data pipelines, ensuring data quality, and enabling analytics for various business units, including marketing and product development. This involved managing data from diverse sources, optimizing storage and processing, and creating tools for governance in a cloud-based environment. My work addressed business challenges like data silos, processing delays, and resource inefficiencies, which hindered timely insights and decision-making across the organization.

- Designed, developed, and maintained three end-to-end ETL pipelines using Apache Airflow and Google BigQuery, aggregating data from various sources such as user events, transaction logs, and external APIs. Implemented partitioning strategies and schema optimizations to enhance query performance, which helped streamline data availability for reporting and analysis, supporting faster campaign evaluations and product iterations.
- Managed ongoing pipeline operations, including task maintenance, data backfills, and incident resolution. Adapted pipelines to evolving business requirements, such as updates to user engagement logic or new event tracking implementations, ensuring data integrity and minimizing disruptions to downstream analytics.
- Created a comprehensive Data Catalog for ETL tables by leveraging generative AI alongside code-based mapping of linear relationships, generating detailed metadata that included table descriptions, dependencies, and usage contexts. This initiative improved data discoverability and governance, facilitating easier collaboration among data teams and reducing time spent on redundant queries.
- Reviewed and consolidated data tables by identifying unused or overlapping ones, merging them into shared sources to eliminate waste and optimize resource allocation. This effort contributed to more efficient storage management and supported cost-effective scaling of data operations.
- Revamped existing dashboards and built new ones in Looker Studio for monitoring key performance indicators, such as user engagement, click-through rates, and voucher redemption trends. Owned the weekly and monthly reporting processes, automating summaries to provide stakeholders with timely, actionable insights that informed marketing strategies and feature launches.
- Performed deep-dive analyses on event tracking data to uncover user behavior patterns, channel performance, and metric fluctuations. Investigated anomalies, calculated impacts from technical incidents, and collaborated with product, engineering, and business teams to refine tracking logic, validate new events, and ensure data accuracy—ultimately aiding in enhanced user retention and engagement strategies.
- Participated in internal innovation challenges, developing a QA chatbot using LLMs, which earned 2nd place in MoMo's Master Challenge for GenAI. Additionally, prepared and delivered training on ETL best practices, fostering knowledge sharing and team development.

This role represented a foundational step in my professional journey, transitioning from internship-level tasks to owning critical data infrastructure components, and highlighted my ability to scale solutions in a high-growth fintech ecosystem.

#### MIS Analyst Intern  
Collectius  
Ho Chi Minh City, Vietnam  
July 2023 – October 2023  

At Collectius, a debt collection and management firm, I served as an MIS Analyst Intern, supporting operational data needs for call centers, finance, and risk management teams. The role involved automating processes, building reporting tools, and analyzing data to optimize collections strategies in a financial services context. Key challenges included manual data handling, delayed reporting, and inefficient customer allocation, which impacted outreach effectiveness and decision-making. My contributions focused on developing automation tools and visualizations to address these issues, drawing from daily tasks that ranged from database setup to ad-hoc support.

- Developed APIs and tools for automating customer allocation to call center teams, incorporating criteria like debt profiles and contact history. This reduced allocation processing time from hours to near real-time, enabling more efficient outreach and helping teams meet daily targets with less manual intervention.
- Engineered SSIS pipelines for daily data updates, automated report generation, and email distribution, integrating data from loan platforms and CRM systems. These enhancements shortened reporting cycles and minimized errors, allowing teams to access fresh insights for same-day strategy adjustments.
- Created Python-based automation scripts for document retrieval, bank statement preprocessing, and fee calculations, handling tasks like filtering relevant data from statements and extracting key variables. This significantly streamlined operational workflows, reducing daily manual efforts and supporting faster resolution of customer cases.
- Designed and deployed Power BI dashboards for sales analysis, KPI tracking, and performance monitoring across five teams serving over 60 employees. Incorporated features like month-over-month comparisons, custom visualizations (using tools like Deneb and Charticulator), and interactive filters, which facilitated data-driven decisions on incentives, targets, and resource allocation.
- Conducted daily monitoring of charts and metrics, detecting anomalies such as payment discrepancies or outreach gaps, and coordinated with departments to investigate root causes. This proactive oversight helped maintain data reliability and supported timely corrections in collections processes.
- Provided ad-hoc support through complex SQL queries for database exploration, report preparation, and data extraction, including lists for waivers, broken contracts, and payment validations. Assisted in mapping data from various sources, updating phone records, and generating forecasts, ensuring operational continuity for partners and internal stakeholders.
- Documented business processes, workflow procedures, and variable definitions during onboarding and throughout the internship, which aided in knowledge transfer and future tool development.

This internship was a pivotal early experience, exposing me to real-world data challenges in finance and building my skills in automation and BI, while demonstrating rapid adaptation from learning company systems to delivering impactful tools.

### Technical Skills

#### Programming Languages
- Python (including libraries such as NumPy, Pandas, PySpark, Matplotlib, Selenium)
- SQL (BigQuery, SQL Server)

#### Data Engineering Tools
- Apache Airflow
- dbt
- SSIS (SQL Server Integration Services)
- GitLab/Git
- Docker 
- n8n 

#### Analytics & Visualization
- Power BI (including Power Query, DAX)
- Looker Studio
- Excel/GG Sheet(advanced querying and automation)
- Equivalent tools: Tableau (for visualization), Superset (open-source BI)

#### Cloud Platforms
- Google Cloud (BigQuery, Cloud Storage, Cloud Functions)
- Microsoft Azure (Data Factory, Fabric, Synapse, Data Lake Storage, DataPipeline, DataFlow, Notebook)
- Equivalent multi-cloud flexibility: Demonstrated adaptability across GCP and Azure for scalable data processing

#### ML/AI Frameworks
- Scikit-learn 
- PyTorch 
- Generative AI: LLMs, RAG, Prompt Engineering
- Equivalent frameworks: TensorFlow , Hugging Face Transformers 

### Key Projects

#### Customer Service Chatbot Optimization  
MoMo (Customer Service Chatbot Team)  
 I led data-driven analytics initiatives to refine the chatbot system. Conducted deep-dive analyses of error logs using large language models (LLMs) for precise categorization and root cause identification, uncovering limitations such as incomplete contextual handling or ambiguous query interpretations. Proposed targeted improvements, including internal tool integrations for real-time transaction validation, service-specific knowledge base mapping, FAQ refinements to eliminate confusion, layered prompt engineering (e.g., general, task-specific, tool-invocation, and response prompts), and feedback loops for service design enhancements. Built real-time monitoring dashboards with automated alerts in Looker Studio and Apache Airflow, while integrating advanced AI techniques like Retrieval-Augmented Generation (RAG) and LLMs for sentiment analysis, issue categorization, and content summarization. These efforts addressed key business challenges by minimizing manual interventions, enhancing response accuracy, and streamlining operations, ultimately leading to improved customer experiences, higher satisfaction scores, and greater operational efficiency.
Tech Stack: Python, SQL (BigQuery), Apache Airflow, Looker Studio, GitLab, n8n

#### Promotion Marketing Platform  
MoMo (CDO Team)  
To support MoMo's marketing campaigns amid growing user data complexity, I developed ETL pipelines aggregating multi-source data for user segmentation and performance tracking. Additionally, conducted in-depth analysis of event parameters from user interactions, such as click events, conversion triggers, and behavioral signals, to uncover patterns and inefficiencies in product features. This analysis informed product improvements by highlighting areas like suboptimal user flows or underperforming elements, leading to targeted refinements that enhanced engagement and conversion rates. Overall, these efforts solved issues of data fragmentation and waste, enabling more precise promotions, informed budgeting decisions, and iterative product enhancements for better user experiences.
Tech Stack: Python, SQL (BigQuery), Looker Studio, GitLab, Generative AI, Google Cloud Storage

#### KPI Tracking and Sales Analysis  
Collectius  
To overcome delays in performance reporting for call centers, I developed SSIS pipelines for daily updates and Power BI dashboards with DAX for interactive KPI visualization. Automated customer allocation APIs further streamlined outreach. These tools tackled manual bottlenecks, facilitating real-time monitoring and better achievement of sales targets.

Tech Stack: SSIS, Power BI, Power Query, DAX, Python, SQL

#### Product Sales Prediction  
Personal Project (GitHub: github.com/duclinhwork/ShopeeSalesForecasting)  
Addressing e-commerce forecasting challenges where variable sales data impacted inventory decisions, I built ML models to predict daily product sales on Shopee. Collected data via web scraping, performed feature engineering, and integrated PhoBERT for review sentiment analysis. This enhanced prediction reliability over baselines, offering practical value for retailers in demand planning.

Tech Stack: Web Scraping (Selenium), XGBoost, Random Forest, LSTM, PhoBERT, Python

#### Cloud ETL for Ad Data Processing  
Personal Project (GitHub: github.com/duclinhwork/Cloud-ETL-for-Ad-Data-Processing)  
In the scenario of fragmented ad performance data causing reporting delays, I created event-driven ETL workflows on Google Cloud to process and load data into BigQuery with transformations like deduplication and aggregation. Developed a Flask API for real-time reporting with external integrations. This resolved inefficiencies in ad analytics, supporting optimized campaign spending and performance evaluation.

Tech Stack: Google Cloud Functions, BigQuery, Python (Pandas), Flask API, Google Cloud Storage

### Education
Master's in Information Systems  
University of Information Technology (UIT) - Vietnam National University, Ho Chi Minh City  
2024 – Present  

Bachelor's in Data Engineering  
Ho Chi Minh City University of Technology and Education (UTE)  
2020 – 2024  
GPA: 8.01 (Distinction)  
Relevant Coursework: Big Data Analysis, Data Visualization, Data Warehousing, Data Mining, Machine Learning (Linear Regression, Logistic Regression, Decision Tree, Random Forest, XGBoost, K-Means, KNN, DBSCAN)

### Certifications & Awards
- 1st Place, MIAI Contest (AI Application Competition)  
- 2nd Place, MoMo’s Internal Master Challenge (GenAI Innovation)  
- AWS Academy Graduate - AWS Academy Cloud Foundations (2022)  
- Data Science Math Skills (2023)  
- HackerRank: SQL (Advanced), Problem Solving (Intermediate)  
- Coursera: Marketing Analytics Foundation, Data Science Math Skills  
- English Proficiency: VSTEP B1 (Level 3/6)